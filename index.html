<!doctype html>
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="author" content="K. Erdem, P. Mazurek, P. Rarus">

    <title>XAI</title>

    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <link rel="stylesheet" href="dist/custom.css">
    <link rel="stylesheet" href="dist/theme/sky.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <!--    <link rel="stylesheet" href="css/highlight/isbl-editor-light.css">-->
    <link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">

</head>
<body>
<div class="reveal">
    <div class="slides">
        <section
          data-background-image="https://images.fineartamerica.com/images-medium-large-5/a-police-officer-in-a-futuristic-smart-car-pulls-paul-noth.jpg"
          data-background-size="contain"
          data-background-color="white"
          data-background-position="left"
          data-transition="none"
        >
            <div
              style="text-align: right; margin-top: 25vh">
            <h2 style="text-align: center; width: 25%; margin-left: auto; white-space: nowrap; font-size: unset; font-weight: bold">Explainable <br/>AI</h2>
            <small style="text-align: center; width: 25%; margin-left: auto; font-size: 0.71em;">Kemal Erdem, Piotr Mazurek, Piotr Rarus</small>

            </div>
        </section>

        <section class="center">
            <h2>Agenda</h2>
            <ul>
                <li>Why XAI and Where XAI?</li>
                <li>Which XAI? (available methods)</li>
                <li>Integrated Gradients</li>
                <li>How to use it?</li>
                <li>Potential issues with XAI methods</li>
            </ul>
        </section>

        <section class="center">
            <span class="heading">Why XAI and Where XAI?</span>
            <p class="display-row">
                <img src="assets/p1/rock.png" style="height: 60vh">
                <small class="caption">Source: AAAI'20 Conference</small>
            </p>
        </section>

        <section class="center">
            <span class="heading">Wrong Decisions might be costly</span>
            <p class="display-row">
                <p  class="display-column">
                    <img src="assets/p1/tesla-crash2.jpg" style="height: 60vh">
                    <img src="assets/p1/tesla-crash1.jpg" style="height: 60vh">
                </p>
                <small class="caption">Source: <a href="https://taiwanenglishnews.com/tesla-on-autopilot-crashes-into-overturned-truck/">https://taiwanenglishnews.com/tesla-on-autopilot-crashes-into-overturned-truck/</a></small>
            </p>
        </section>

        <section class="center">
            <span class="heading">But XAI can help us improve models</span>
            <p class="display-row">
                <img src="assets/p1/improve-model.png" style="height: 60vh">
                <small class="caption">Source: W. Samek, A. Binder, MICCAIâ€™18</small>
            </p>
        </section>

        <section class="center">
            <span class="heading">And we can learn from it</span>
            <p class="display-row">
                <img src="assets/p1/alphago-move.gif" style="height: 70vh">
                <small class="caption">Source: <a href="https://www.youtube.com/watch?v=WXuK6gekU1Y">Google DeepMind beating world champion Lee Se-Dol at GO</a></small>
            </p>
        </section>

        <section class="center">
            <span class="heading">The Law</span>
            <p>
                <img src="assets/p1/california-act.png" style="height: 20vh">
                <img src="assets/p1/gdpr.jpg" style="height: 20vh">
                <img src="assets/p1/congress.png" style="height: 20vh">
            </p>
            <blockquote style="font-size: 0.8em">"Article 22 of <b>GDPR</b> empowers individuals with
                the <b>right to demand an explanation of how
                an AI system made a decision that affects
                    them</b>" - <span style="text-transform: none !important; font-style: normal !important;">European Commission</span></blockquote>
            <blockquote style="font-size: 0.8em">"Provide an <b>assessment of the risks posed by
                the automated decision system</b> to the privacy or security and the <b>risks that contribute to inaccurate, unfair,
                biased, or discriminatory decisions</b> impacting consumers" - <span style="text-transform: none !important; font-style: normal !important;">Algorithmic Accountability Act 2019</span></blockquote>
        </section>

        <section class="center">
            <span class="heading">Which XAI?</span><br/><br/>
            <ul>
                <li>Surrogate Model Based </li>
                <li>Attribution Based</li>
                <li>Contrastive Explanations</li>
                <li>Counterfactual / Recourse Based</li>
                <li>Example similarity</li>
            </ul>
        </section>
        <section class="center">
            <span class="heading">Surrogate Model Based Explanation</span><br/><br/>
            <p class="display-row">
                <img src="assets/p1/black-box.png" style="height: 60vh">
                <small class="caption">Black Box model</small>
            </p>
        </section>
        <section class="center">
            <span class="heading">Surrogate Model Based Explanation</span><br/><br/>
            <p class="display-row">
                <img src="assets/p1/global-surrogate.png" style="height: 60vh">
                <small class="caption">Global Surrogate model</small>
            </p>
        </section>
        <section class="center">
            <span class="heading">Surrogate Model Based Explanation</span><br/><br/>
            <p class="display-row">
                <img src="assets/p1/complex-tree.png" style="height: 60vh">
                <small class="caption">Source: <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.270.2463&rep=rep1&type=pdf">J48 and VTJ48 decision trees doi:10.1.1.270.2463</a></small>
            </p>
        </section>
        <section class="center">
            <span class="heading-secondary">LIME - Local Interpretable Model agnostic Explanations</span><br/><br/>
            <div class="r-stack">
                <p class="display-row no-margin">
                    <img src="assets/p1/lime1.png" style="height: 60vh">
                    <small class="caption">Source: <a href="https://christophm.github.io/interpretable-ml-book/index.html">Interpretable Machine Learning - Christoph Molnar</a></small>
                </p>
                <p class="display-row no-margin fragment">
                    <img src="assets/p1/lime2.png" style="height: 60vh">
                    <small class="caption">Source: <a href="https://christophm.github.io/interpretable-ml-book/index.html">Interpretable Machine Learning - Christoph Molnar</a></small>
                </p>
                <p class="display-row no-margin fragment">
                    <img src="assets/p1/lime3.png" style="height: 60vh">
                    <small class="caption">Source: <a href="https://christophm.github.io/interpretable-ml-book/index.html">Interpretable Machine Learning - Christoph Molnar</a></small>
                </p>
                <p class="display-row no-margin fragment">
                    <img src="assets/p1/lime4.png" style="height: 60vh">
                    <small class="caption">Source: <a href="https://christophm.github.io/interpretable-ml-book/index.html">Interpretable Machine Learning - Christoph Molnar</a></small>
                </p>
            </div>
        </section>
        <section class="center">
            <span class="heading">Attribution Based Explanation</span><br/><br/>
            <p class="display-row">
                <span>You have access to the model's structure</span>
            </p>
        </section>
        <section class="center">
            <span class="heading">Integrated Gradients</span><br/><br/>
            <p class="display-row" style="font-size: 0.8em;">
                $$
                IntegratedGrads_{i}(x) ::= (x_{i} - x'_{i})\times\int_{\alpha=0}^1\frac{\partial F(x'+\alpha \times (x - x'))}{\partial x_i}{d\alpha}
                $$
                $$
                IntegratedGrads^{approx}_{i}(x)::=(x_{i}-x'_{i})\times\sum_{k=1}^{m}\frac{\partial F(x' + \frac{k}{m}\times(x - x'))}{\partial x_{i} } \times \frac{1}{m}
                $$
            </p>
            <small class="caption">Source: <a href="https://arxiv.org/abs/1703.01365">Axiomatic Attribution for Deep Networks - M. Sundararajan, A. Taly, Q. Yan</a></small>
        </section>
        <section class="center">
            <span class="heading">Integrated Gradients - variables</span><br/><br/>
            <ul>
                <li>$i$ - feature</li>
                <li>$x$ - input (image)</li>
                <li>$x'$ - baseline (image)</li>
                <li>$\alpha$ - interpolation constant</li>
                <li>$k$ - scaled interpolation constant</li>
                <li>$m$ - number of steps</li>
                <li>$(x_{i}-x'_{i})$ - scale factor</li>
                <li>$F()$ - model's prediction function</li>
                <li>$\frac{\partial F}{\partial x_{i} }$ - gradient relative to feature $x_i$</li>
            </ul>
        </section>
        <section class="center">
            <span class="heading">Test subjects</span><br/><br/>
            <div class="display-row">
                <p>
                    <img src="assets/p1/gp-rab.png" style="height: 60vh">
                    <img src="assets/p1/gp-bee.png" style="height: 60vh">
                </p>
                <small class="caption">Source: <a href="https://www.petsmart.com/small-pet/toys-and-habitat-accessories/small-pet-costumes/">https://www.petsmart.com/small-pet/toys-and-habitat-accessories/small-pet-costumes/</a></small>
            </div>
        </section>
        <section class="center">
            <span class="heading">Interpolation</span><br/><br/>
            <div class="display-row">
                <div style="display: flex">
                    <img src="assets/p1/baseline-img.png" style="height: 30vh" />
                    <div style="width: 50vw; font-size: 0.4em; padding-top: 5vh;">$$IntegratedGrads^{approx}_{i}(x)::=(x_{i}-x'_{i})\times\sum_{k=1}^{m}\frac{\partial F(\overbrace{x' + \frac{k}{m}\times(x - x')}^\text{interpolate m images at k intervals})}{\partial x_{i} } \times \frac{1}{m}$$
                    </div>
                </div>
                <img src="assets/p1/gp-alpha.png" style="height: 30vh" />
                <small class="caption">Interpolation process</small>
            </div>
        </section>
        <section class="center">
            <span class="heading">Compute Gradients</span><br/><br/>
            <div class="display-row">
                <div style="font-size: 0.8em;">
                    $$
                    IntegratedGrads^{approx}_{i}(x)::=(x_{i}-x'_{i})\times\sum_{k=1}^{m}\frac{\overbrace{\partial F(\text{interpolated images})}^\text{compute gradients} }{\partial x_{i} } \times \frac{1}{m}
                    $$
                </div>
                <img src="assets/p1/grads.png" style="height: 30vh" />
            </div>
        </section>
        <section class="center">
            <span class="heading">Compute Gradients</span><br/><br/>
            <div class="display-row">
                <div style="font-size: 0.6em;">
                    $$
                    IntegratedGrads^{approx}_{i}(x)::=(x_{i}-x'_{i})\times \overbrace{\sum_{k=1}^{m} }^\text{Sum m local gradients}
                    \text{gradients(interpolated images)} \times \overbrace{\frac{1}{m} }^\text{Divide by m steps}
                    $$
                </div>
            </div>
        </section>
        <section class="center" data-background-color="white">
<!--            <span class="heading">Results</span><br/><br/>-->
            <div class="r-stack display-row">
                <div style="margin-left: 10vw; margin-top: 10vh; font-size: 1.4em">
                    <span style="width: 25vw">Results</span>
                </div>
                <div class="display-column fragment fade-in-then-out" data-transition="none">
                    <span style="width: 25vw">Class: <b>"guinea pig"</b></span>
                    <img src="assets/p1/gp-gp-pred.png" style="height: 90vh" />
                </div>
                <div class="display-column fragment fade-in-then-out" data-transition="none">
                    <span style="width: 25vw">Class: <b>"bee"</b></span>
                    <img src="assets/p1/gp-bee-pred.png" style="height: 90vh" />
                </div>
                <div class="display-column fragment fade-in-then-out" data-transition="none">
                    <span style="width: 25vw">Class: <b>"guinea pig"</b></span>
                    <img src="assets/p1/rab-gp-pred.png" style="height: 90vh" />
                </div>
                <div class="display-column fragment fade-in-then-out" data-transition="none">
                    <span style="width: 25vw">Class: <b>"hare"</b></span>
                    <img src="assets/p1/rab-rab-pred.png" style="height: 90vh" />
                </div>
            </div>
        </section>
        <section class="center">
            <span class="heading">Contrastive Explanations</span><br/><br/>
            <span >Why Spoonbill, rather than Flamingo?</span><br/><br/>
            <p class="display-row no-margin">
                <div>
                    <img src="assets/p1/contrastive-gradcam.png" style="height: 50vh" />
                    <img src="assets/p1/contrastive-example.png" style="height: 50vh" />
                </div>
                <small class="caption">Source: <a href="https://arxiv.org/abs/2008.00178">Contrastive Explanations in Neural Networks - 2020 - M. Prabhushankar, G. Kwon, D. Temel, G. AlRegib</a></small>
            </p>
        </section>
        <section class="center">
            <span class="heading" style="font-size: 1.2em">Counterfactual / Recourse Based Explanations</span><br/><br/>
            <span>How to change X to predict Y?</span><br/><br/>
            <p class="display-row no-margin">
                <img src="assets/p1/counterfactual.png" style="height: 50vh" />
                <small class="caption">Source: <a href="https://arxiv.org/abs/2009.05487">Counterfactual Explanations & Adversarial Examples -- Common Grounds, Essential Differences, and Potential Transfers - 2020 - T. Freiesleben</a></small>
            </p>
        </section>
        <section class="center">
            <span class="heading">Example similarity Explanations</span><br/><br/>
            <p class="display-row">
                <img src="assets/p1/example-based.png" style="height: 60vh">
                <small class="caption">Author: <a href="https://twitter.com/karpathy">Andrej Karpathy</a>, generated from <a href="http://www.image-net.org/challenges/LSVRC/2014/">ILSVRC 2012 images</a></small>
            </p>
        </section>

        <section class="center" data-auto-animate>
            <h2>How to use it</h2>
            <h4>Available Tools</h4>
        </section>

        <section class="center" data-auto-animate>
            <h2>How to use it</h2>
            <h4>Available Tools</h4>
            <ol class="small">
                <li>Captum</li>
                <li>tf-explain</li>
                <li>Lime</li>
                <li>SHAP</li>
            </ol>
        </section>

        <section>
            <h2>Captum</h2>
            <ul>
                <li>Dedicated to interpret torch-based models</li>
                <li>Developed by Facebook under Facebook Open Source</li>
                <li>Available on pypi</li>
                <li>Used to develop and troubleshoot models</li>
                <li>Can be used on production to help users understand model's prediction</li>
            </ul>
        </section>

        <section>
            <h2>Available Methods</h2>
            <ul>
                <li>Primary Attribution: Evaluates contribution of each input feature to the output of a model.</li>
                <ul class="small">
                    <li>Integrated Gradients</li>
                    <li>Gradient SHAP</li>
                    <li>Saliency</li>
                    <li>Guided Backpropagation, Deconvolution</li>
                    <li>Guided GradCAM</li>
                </ul>
                <li>Layer Attribution: Evaluates contribution of each neuron in a given layer to the output of the model.</li>
                <ul class="small">
                    <li>Layer Conductance</li>
                    <li>Internal Influence</li>
                    <li>Layer Activation</li>
                    <li>GradCAM</li>

                </ul>
                <li>Neuron Attribution: Evaluates contribution of each input feature on the activation of a particular hidden neuron.</li>
                <ul class="small">
                    <li>Neuron Conductance</li>
                    <li>Neuron Integrated Gradients</li>
                    <li>Neuron GradientSHAP</li>
                </ul>
            </ul>
        </section>

        <section data-background-iframe="https://captum.ai/">
        </section>

        <section>
            <h2>Example</h2>
            <pre><code data-trim data-noescape class="python">
                from captum.attr import GuidedGradCam
                from torchvision import models

                # Load pretrained AlexNet
                alexnet = models.alexnet(pretrained=True)
                alexnet.eval()

                # Create object to interpret the model
                # To make GradCam work we pass reference to last conv layer
                guided_gc = GuidedGradCam(alexnet, alexnet.features[10])

                # Predict output on data
                out = alexnet(batch)

                # Point out classes with highest score
                score, index = out.max(1)

                # Use interpreter to calculate attributions
                attributions = guided_gc.attribute(batch_t, index)
            </code></pre>
        </section>

        <section>
            <h2>Captum Insights</h2>
            <pre><code data-trim data-noescape class="python">
                import torch
                import torch.nn.functional as F
                from captum.insights import AttributionVisualizer
                from captum.insights.features import ImageFeature
                from torchvision import models

                # Load pretrained AlexNet
                alexnet = models.alexnet(pretrained=True)
                alexnet.eval()

                # Launch visualization inside the notebook
                visualizer = AttributionVisualizer(
                    models=[alexnet],
                    score_func=lambda o: F.softmax(o, 1),
                    classes=[...],  # class labels
                    features=[
                        ImageFeature(
                            "Photo",
                            baseline_transforms=[...],
                            input_transforms=[...],
                        )
                    ],
                    dataset=...,
                )

                visualizer.render()
            </code></pre>
        </section>

        <section>
            <h2>Captum Insights</h2>
            <img src="assets/tools/captum_insights.png">
        </section>

        <section>
            <h2>tf-explain</h2>
            <ul>
                <li>Dedicated to interpret TensorFlow-based models</li>
                <li>No official backing from a large company</li>
                <li>Available on pypi</li>
                <li>Used to develop and troubleshoot models</li>
                <li>Can be used on production to help users understand model's prediction</li>
            </ul>
        </section>

        <section>
            <h2>Available Methods</h2>
            <ul>
                <li>Activations Visualization</li>
                <li>Vanilla Gradients</li>
                <li>Gradients*Inputs</li>
                <li>Occlusion Sensitivity</li>
                <li>Grad CAM</li>
                <li>SmoothGrad</li>
                <li>Integrated Gradients</li>
            </ul>
        </section>

        <section data-background-iframe="https://tf-explain.readthedocs.io/en/latest/">
        </section>

        <section>
            <h2>Usage - Core API</h2>
            <pre><code data-trim data-noescape class="python">
                from tf_explain.core.grad_cam import GradCAM

                explainer = GradCAM()

                output = explainer.explain(*explainer_args)

                explainer.save(output, output_dir, output_name)
            </code></pre>
        </section>

        <section>
            <h2>Usage - Callbacks</h2>
            <pre><code data-trim data-noescape class="python">
                from tf_explain.callbacks.grad_cam import GradCAMCallback

                callbacks = [
                    GradCAMCallback(
                        validation_data=(x_val, y_val),
                        layer_name="activation_1",
                        class_index=0,
                        output_dir=output_dir,
                    )
                ]
                
                model.fit(x_train, y_train, batch_size=2, epochs=2, callbacks=callbacks)
            </code></pre>
        </section>

        <section>
            <h2>Lime</h2>
            <ul>
                <li>Applicable to any black-box classifier</li>
                <li>Works on text, tabular and image data</li>
                <li>Available on pypi</li>
            </ul>
        </section>

        <section data-background-iframe="https://github.com/marcotcr/lime">
        </section>

        <section>
            <h2>Usage</h2>
            <pre><code data-trim data-noescape class="python">
                from lime import lime_image

                explainer = lime_image.LimeImageExplainer()
                
                explanation = explainer.explain_instance(
                    image, # input image
                    predict, # predict function of interpreted classifier
                )
            </code></pre>
            <div>
                <table>
                    <tr>
                        <td>
                            <p class="display-row">
                                <img src="assets/tools/lime_example_1.png">
                                <small class="caption">Input image</small>
                            </p>   
                        </td>
                        <td>
                            <p class="display-row">
                                <img src="assets/tools/lime_example_2.png">
                                <small class="caption">Interpretation of dog prediction</small>
                            </p>
                        </td>
                    </tr>
                </table>
                <small class="caption">Green areas are contributing positively to prediction, red areas have negative effect on prediction</small>
            </div>
        </section>

        <section data-background-iframe="https://github.com/slundberg/shap">
        </section>

        <section>
            <h2>DeepShap - Usage</h2>
            <pre><code data-trim data-noescape class="python">
                import numpy as np
                import shap

                model = ... # classifier model
                images = ... # image date set
                background = images[:100]
                test_images = images[100:103]

                explainer = shap.DeepExplainer(model, background)
                shap_values = e.shap_values(test_images)

                shap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]
                test_numpy = np.swapaxes(np.swapaxes(test_images.numpy(), 1, -1), 1, 2)

                shap.image_plot(shap_numpy, -test_numpy)
            </code></pre>
        </section>

        <section>
            <h2>DeepShap - Example</h2>
            <p class="display-row">
                <img src="assets/tools/shap_example_1.png" style="height: 45vh">
                <small class="caption">Contribution of features to each class prediction</small>
            </p>
        </section>

        <section>
            <h2>SHAP - Gradient Explainer</h2>
            <small class="heading">Combines Integrated Gradients, SHAP and SmoothGrad into single interpretation method.</small>
            <p class="display-row">
                <img src="assets/tools/shap_example_2.png" style="height: 60vh">
                <small class="caption">Contribution of features to 2 top predictions</small>
            </p>
        </section>

        <section class="center">
            <h2>XAI challenges</h2>
            <h4>How to evaluate explanation?</h4>
            <h4>Why one explanation is better then another</h4>
        </section>

        <section data-background-iframe="https://distill.pub/2020/attribution-baselines/">
        </section>

        <section class="center">
            <h2>How to Evaluate XAI?</h2>
            <a href="https://github.com/SinaMohseni/Awesome-XAI-Evaluation#how-to-evaluate-xai"><img
                    src="https://raw.githubusercontent.com/SinaMohseni/Awesome-XAI-Evaluation/master/figures/target-users.png"></a>
        </section>

        <section data-background-iframe="https://distill.pub/2020/attribution-baselines/">
        </section>

        <section class="center">
            <h2>Two distinct approaches</h2>
            <h4>Human-grounded Measures</h4>
            <h4>Computational Measures</h4>
        </section>

        <section class="center">
            <h2>Human-grounded Measures</h2>
            <h4>Metrics like IoU</h4>
            <h4>Researchers conduct surveys where they ask: "which part of data is important"   </h4>
        </section>

        <section data-background-iframe="https://arxiv.org/pdf/1801.05075.pdf">
        </section>

        <section class="center">
            <h2>Computational Measures</h2>
            <h4>Dozens of approaches </h4>
            <h4>No established standards</h4>
            <h4>Open research area</h4>
        </section>


        <section data-background-iframe="https://arxiv.org/pdf/1806.10758.pdf">
        </section>


        <section class="center">
            <h2>Infidelity</h2>
            <h4>How much explanation differ given a perturbed input image</h4>
        </section>

        <section data-background-iframe="https://arxiv.org/pdf/1901.09392.pdf">
        </section>

        <section data-background-iframe="https://captum.ai/api/metrics.html#infidelity">
        </section>


        <section class="center">
            <h2>Infidelity</h2>

            <pre data-id="code-animation"><code class="python" data-trim data-line-numbers="">

net = ImageClassifier()
saliency = Saliency(net)

# Computes saliency maps for class 3 for the input image.
attribution = saliency.attribute(input, target=3)

# define a perturbation function for the input
def perturb_fn(inputs):
    noise = torch.tensor(np.random.normal(0, 0.003, inputs.shape)).float()
    return noise, inputs - noise

infidelity(net, perturb_fn, input, attribution)
>> 0.2177
					</code></pre>
        </section>

        <section class="center">
            <h2>Other</h2>
            <h4>We remove "important" inputs and network confidence should decrease</h4>
        </section>

        <section class="center">
            <h2>XAI challenges</h2>
            <h4>How to explain a wrong prediction</h4>
            <img src="assets/wrong_pred.png">
        </section>

        <section class="center">
            <h2>Wrong prediction</h2>
            <h4>Explanation makes even less sense</h4>
            <img src="assets/wrong_pred_cats.png">
        </section>

        <section class="center">
            <h2>XAI challenges</h2>
            <h4>Hyperparameters have a massive impact on a given explanation</h4>
        </section>

        <section data-background-iframe="https://distill.pub/2020/attribution-baselines/">
        </section>

        <section class="center">
            <h2>Ok, I'm hyped</h2>
            <h4>Where I should start my PhD</h4>
        </section>

        <section data-background-iframe="https://hai.cs.washington.edu/">
        </section>



        <section data-background-iframe="http://pbiecek.github.io/jobs.html">
        </section>



        <section class="center">
            <h2>TLDR</h2>
            <ul>
                <li>XAI methods doesn't always work great</li>
                <li>but still we need them to trust ML systems</li>
                <li>Available out-of-the-box implementations that will work with your model</li>
                <li>An open problem, many challenges</li>
            </ul>
        </section>

        <section class="center">
            <h3>References</h3>
            <ul class="small">
                <li><a href="https://arxiv.org/abs/1602.04938">"Why Should I Trust You?": Explaining the Predictions of Any Classifier - 2016, M. Ribeiro, S. Singh, C. Guestrin</a></li>
                <li><a href="https://arxiv.org/abs/1703.01365">Axiomatic Attribution for Deep Networks - 2017, M. Sundararajan, A. Taly, Q. Yan</a></li>
                <li><a href="https://christophm.github.io/interpretable-ml-book/index.html">Interpretable Machine Learning - Christoph Molnar</a></li>
                <li><a href="https://arxiv.org/abs/2008.00178">Contrastive Explanations in Neural Networks - 2020 - M. Prabhushankar, G. Kwon, D. Temel, G. AlRegib</a></li>
                <li><a href="https://arxiv.org/abs/2009.05487">Counterfactual Explanations & Adversarial Examples -- Common Grounds, Essential Differences, and Potential Transfers - 2020 - T. Freiesleben</a></li>
                <li><a href="https://cs.stanford.edu/people/karpathy/cnnembed/">ILSVRC 2012 images visualization by Andrej Karpathy</a></li>
            </ul>
        </section>

        <section class="center">
            <h2>Thanks</h2>
            <blockquote>"There's no such thing as a stupid question!"</blockquote>
            <p>
                <small>Kemal Erdem, Piotr Mazurek, Piotr Rarus</small><br/>
                <small>Presentation avalibe at: <a href="https://tugot17.github.io/XAI-Presentation/#/">https://tugot17.github.io/XAI-Presentation</a></small><br/>
            </p>
        </section>
    </div>
</div>


<script src="dist/reveal.js"></script>
<script src="plugin/notes/notes.js"></script>
<script src="plugin/markdown/markdown.js"></script>
<script src="plugin/highlight/highlight.js"></script>
<script src="plugin/math/math.js"></script>
<script>
    // More info about config & dependencies:
    // - https://github.com/hakimel/reveal.js#configuration
    // - https://github.com/hakimel/reveal.js#dependencies
    Reveal.initialize({
        hash: true,
        controls: true,
        progress: true,
        slideNumber: true,
        overview: true,
        center: false,
        navigationMode: 'default',
        fragmentInURL: false,
        embedded: false,
        preloadIframes: null,
        autoSlide: 0,
        autoSlideStoppable: true,
        defaultTiming: 120,
        mouseWheel: false,
        previewLinks: false,
        width: '100%',
        height: '100%',
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        transitionSpeed: 'fast', // default/fast/slow
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        display: 'block',
        math: {
            mathjax: 'plugin/math/MathJax.js',
            config: 'TeX-AMS_HTML-full', // See http://docs.mathjax.org/en/latest/config-files.html
            // pass other options into `MathJax.Hub.Config()`
            TeX: {Macros: {RR: "{\\bf R}"}}
        },
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
    });
</script>
</body>
</html>
